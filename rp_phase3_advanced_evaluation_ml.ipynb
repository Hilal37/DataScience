{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RP Phase 3\n",
    "## Advanced Evaluation of ML Classification Models\n",
    "## RP Group 1\n",
    "\n",
    "While the paper focuses on binary classification problems, our problem is multi-label, with over 200 output classes, where each input record can belong to more than one output class. So, implementing the paper's methodology as-is will not work for the whole problem. A close analog to the paper's method is to analyze the same performance metrics and model behavior, but on one output class at a time.\n",
    "\n",
    "So, each class will have its own results and plots: its own set of precision/recall curves, its own empirical risk curves, its own model behavior interpretations and so on. In this notebook, we only focus on getting these results for the first output class. Similar work can be donw for the other 200 or so classes.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "We use pickle so that we can re-use the same train/test split that we originally trained the two models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"X_train.pickle\", \"rb\") as f1, open(\"X_test.pickle\", \"rb\") as f2, \\\n",
    "    open(\"y_train.pickle\", \"rb\") as f1_y, open(\"y_test.pickle\", \"rb\") as f2_y:\n",
    "    X_train = pickle.load(f1)\n",
    "    X_test = pickle.load(f2)\n",
    "    y_train = pickle.load(f1_y)\n",
    "    y_test = pickle.load(f2_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models\n",
    "\n",
    "The only two models we have that were trained on the same split are a Random Forest model and a Linear SVM. Re-traninig the other models would take days to finish, given the size of our dataset and the large number of output classes of our multi-label problem.\n",
    "\n",
    "We have previously-saved pickle files of these models, hoping we can re-use them. However to our surprise, the Random Forest file is awfully large (4.5 GB), and we are unable to load it into memory.\n",
    "\n",
    "As most of the work requires comparing performance/behavior of multiple models, we will only show code we would have used here, without output sadly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file too large (uncompressed is 4.5 GB), unable to load into memory\n",
    "#even tried to compress it, but load() de-compresses it and we ge the same problem\n",
    "from joblib import load\n",
    "\n",
    "rf_clf = load(\"clf_rf.joblib.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clf.pickle\", \"rb\") as f:\n",
    "    svm_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Notice that from now on, we only look at the first column of the predictions matrix of X_test. This is to only focus on the first output class, its results, plots, and model behavior for this particular class. \n",
    "\n",
    "### a- Traditional Metrics\n",
    "\n",
    "We first assess model performance using some traditional metrics: precision, recall, and ROC curves. For each model, these metrics are computed, precision/recall curves are plotted, same for ROC curves.\n",
    "\n",
    "However, these metrics alone are not enough for assessing the quality of probabilities, since these metrics only look at the quality of cassification splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traditional metrics: Precision, Recall, ROC curves\n",
    "#on binary predictions, not probabilistic\n",
    "\n",
    "#TODO\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#only look at predictions of the first output class (notice the [:, 0] => first column only)\n",
    "predictions = {\n",
    "    'Random Forest': np.array(rf_clf.predict(X_test))[:, 0],\n",
    "    'SVM': np.array(svm_clf.predict(X_test))[:, 0]\n",
    "}\n",
    "\n",
    "precisions = {k: precision_score(y_true=y_test, y_pred=v) for k, v in predictions.items()}\n",
    "recalls = {k: recall_score(y_true=y_test, y_pred=v) for k, v in predictions.items()}\n",
    "\n",
    "print(precisions)\n",
    "print(recalls)\n",
    "\n",
    "#ROC curves\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "models = [rf_clf, svm_clf]\n",
    "for i in range(len(models)):\n",
    "    plot_roc_curve(models[i], X_test, y_test, ax=ax, name=list(predictions.keys())[i])\n",
    "    \n",
    "plt.title(\"ROC Curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting precision/recall results\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "modelNames = list(precisions.keys())\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "ax[0].barh(np.arange(len(modelNames)), precisions.values(),\n",
    "                     align='center',\n",
    "                     height=0.5,\n",
    "                     tick_label=modelNames)\n",
    "\n",
    "ax[0].set_title(\"Precision\")\n",
    "\n",
    "ax[1].barh(np.arange(len(modelNames)), recalls.values(),\n",
    "                     align='center',\n",
    "                     height=0.5,\n",
    "                     tick_label=modelNames)\n",
    "\n",
    "ax[1].set_title(\"Recall\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Empirical Risk Curve\n",
    "\n",
    "This is a metric that does reveal the quality of probabilistic predictions and observation rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empirical risk curve for each model\n",
    "#on probabilistic predictions\n",
    "import copy\n",
    "\n",
    "# predictions_proba = {\n",
    "#     'Random Forest': [x[1] for x in rf_clf.predict_proba(X_test)],\n",
    "#     'SVM': [x[1] for x in svm_clf.predict_proba(X_test)]\n",
    "# }\n",
    "\n",
    "# also here, we only focus on the first column of results\n",
    "# (predict_proba returns a 3d matrix, so we slice it this way)\n",
    "predictions_proba = {\n",
    "    'Random Forest': [x[1] for x in [y[0] for y in rf_clf.predict_proba(X_test)]],\n",
    "    'SVM': [x[1] for x in [y[0] for y in svm_clf.predict_proba(X_test)]]\n",
    "}\n",
    "\n",
    "def empirical_risk(l):\n",
    "    l_pos = copy.deepcopy(l)\n",
    "    l_pos = [(i, l_pos[i]) for i in range(len(l_pos))]\n",
    "    l_pos = sorted(l_pos, key=lambda x: x[1])\n",
    "    \n",
    "    nbins=10\n",
    "    items_per_bin = len(l_pos) // nbins\n",
    "    \n",
    "    res = []\n",
    "    for i in range(nbins):\n",
    "        lsub = l_pos[i*items_per_bin: (i+1)*items_per_bin]\n",
    "        res += [sum([y_test[idx] for idx, _ in lsub])/items_per_bin]\n",
    "    \n",
    "    return res\n",
    "\n",
    "empirical_risks = {k: empirical_risk(list(v)) for k, v in predictions_proba.items()}\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for k, v in empirical_risks.items():\n",
    "    plt.plot([1, 2, 3, 4 ,5, 6, 7, 8, 9, 10], v, label=k)\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Empirical Risk Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c- Precision/Recall at Top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision and recall at top k, for varying values of k\n",
    "\n",
    "ranked_lists = {k: sorted([(i, v[i]) for i in range(len(v))], key=lambda x: x[1], reverse=True) \\\n",
    "                for k, v in predictions_proba.items()}\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "k_values = [5, 10] + list(range(20, len(y_test), 20))\n",
    "\n",
    "for model, ranked_list in ranked_lists.items():\n",
    "    # (the [y_test[x[0]] for x in ranked_lists[:k]] part is the ground truth at top-k)\n",
    "    precisions = [sum([y_test[x[0]] for x in ranked_list[:k]])/k for k in k_values]\n",
    "    recalls = [sum([y_test[x[0]] for x in ranked_list[:k]])/sum(y_test) for k in k_values]\n",
    "    \n",
    "    ax[0].plot(k_values, precisions, label=model)\n",
    "    ax[1].plot(k_values, recalls, label=model)\n",
    "    \n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "### a- Feature Importance\n",
    "\n",
    "To better understand model behavior, it is important to see which features each model relies the most on. FOr this, feature importance is computed for each feature and model, and the top 5 most important features for each model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 5 most important features from each model\n",
    "#see if they are relevant\n",
    "\n",
    "#feature importances of each model\n",
    "\n",
    "feature_imp = {\n",
    "    \"Random Forest\": rf_clf.best_estimator_.feature_importances_,\n",
    "    \"SVM\": np.std(X_train, 0)*svm_clf.best_estimator_.coef_[0]\n",
    "}\n",
    "\n",
    "def getTopFiveFeatures(importances):\n",
    "    #column names after the transforms\n",
    "    colnames = range(1, len(importances)+1)\n",
    "    \n",
    "    importances_names = [(colnames[i], importances[i]) for i in range(len(importances))]\n",
    "    \n",
    "    importances_names = sorted(importances_names, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [x[0] for x in importances_names[:5]]\n",
    "\n",
    "print({k: getTopFiveFeatures(list(v)) for k, v in feature_imp.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Mistake Patterns\n",
    "\n",
    "We now shift our attention to possible mistake patterns the models may have, more precisely the top-5 mistake patterns of each model. This helps us get a better understanding of where the models are being confused the most, and possibly act on these problematic patterns (and/or the model itself) to improve future model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all frequent patterns using FP-growth technique\n",
    "#and then get the top-R \"mistake patterns\", i.e. those R patterns with the highest proba of mistake\n",
    "\n",
    "from fp_growth import find_frequent_itemsets\n",
    "\n",
    "X_test_with_idx = [[i] + X_test[i] for i in range(len(X_test))]\n",
    "\n",
    "models = [rf_clf, svm_clf, dt_clf, lr_clf, ada_boost_clf]\n",
    "\n",
    "#frequent patterns in data\n",
    "frequent_patterns = find_frequent_itemsets(X_test_with_idx, 0.1)\n",
    "nb_freq_patterns = sum(1 for _ in frequent_patterns)\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    print(\"Results for model\", model.best_estimator_)\n",
    "    \n",
    "    mistake_rates = [(idx, get_accuracy(model, frequent_patterns[idx])) for idx in range(nb_freq_patterns)]\n",
    "    \n",
    "    print(sorted(mistake_rates, key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "def get_accuracy(model, pattern):\n",
    "    y_true = [y_test[x[0]] for x in pattern]\n",
    "    y_pred = model.predict(pattern[:, 1:])\n",
    "    return accuracy_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_patterns = find_frequent_itemsets(X_test, 0.1)\n",
    "sum(1 for _ in frequent_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c- Model Comparison\n",
    "\n",
    "Two models that output the same results are definitely redundant. This is why we are interested in comparing the ranked results returned by each of the models, to see how (dis)similar two models are.\n",
    "\n",
    "For this, we plot Jaccard Similarities at top-k for each pair of models, just like the paper did. We use our test data and its predictions for these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the results of the top-k highest risk of rejection applicants\n",
    "#see how (dis)similar the result sets of the models are\n",
    "#use Jaccard similarity at top-k\n",
    "\n",
    "# For sets:\n",
    "# | operator is union\n",
    "# & operator is intersection\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def jaccard_similarity(l1, l2):\n",
    "    if len(set(l1) | (set(l2))) == 0:\n",
    "        return 0\n",
    "    return len(set(l1) & (set(l2))) / len(set(l1) | (set(l2)))\n",
    "\n",
    "ranked_lists = {k: sorted([(i, v[i]) for i in range(len(v))], key=lambda x: x[1], reverse=True) \\\n",
    "                for k, v in predictions_proba.items()}\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(1, figsize=(15, 6))\n",
    "\n",
    "k_values = [5, 10] + list(range(20, len(y_test), 20))\n",
    "\n",
    "i = 0\n",
    "for model1, ranked_list1 in ranked_lists.items():\n",
    "    for model2, ranked_list2 in list(ranked_lists.items())[i:]:\n",
    "        if model1 != model2:\n",
    "            \n",
    "            plt.plot(k_values, [jaccard_similarity([x[0] for x in ranked_list1[:k]], \\\n",
    "                                              [x[0] for x in ranked_list2[:k]], \\\n",
    "                                              )\n",
    "                               for k in k_values], label=f\"{model1} - {model2}\")\n",
    "            \n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of children of FP-tree (corresponds to n of sets found)\n",
    "fp_tree.root.children.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
